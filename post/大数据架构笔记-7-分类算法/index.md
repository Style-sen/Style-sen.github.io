# 大数据架构笔记-7-分类算法


## 朴素贝叶斯（NB）

1. 在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。
2. `主体`、`特征`和`类别`三个元素。
3. 主体和类别好确定。特征的值不好确定（离散值）【连续值通过划分区间可变为离散值】【少的样本可通过正态分布获得特征值】

### 优点：

1. 简单、易实现、适合文本。
2. 容易并行。

### 缺点：

1. 独立性假设。

### 模型

概率值。p(y)  p(x|y)

### 实践（文章分类）

#### 数据预处理

1. 样本拆分训练集和测试集，并且ID化。

#### 训练模型

#### 测试

## 决策树（DT）

1. 直接使用决策树解决分类问题，实际工作中不是很常用。
2. 但是经常会使用他衍生出来的其他算法（randforest/gbdt等）。

### 信息熵、香农定理

1. 描述混乱程度、纯度、不确定度

#### 信息量

1. 信息量是对信息的度量。
2. 信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大；越大概率的事情发生了产生的信息量越小。
3. 信息量的公式： 𝐡(𝐱) = −𝒍𝒐𝒈𝟐𝒑(𝒙)   ，【负号是为了确保信息一定是正数或者是 0，总不能为负数；普遍传统，使用 2 作为对数的底】。
4. 信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。
𝐇(𝐱) = −𝒔𝒖𝒎(𝒑(𝒙)𝒍𝒐𝒈𝟐𝒑(𝒙))
5. 信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多， 那么他的信息熵是比较大的。
6. 如果一个系统越简单，出现情况种类很少（极端情况为 1 种情况，那么对应概率为 1，那么对应的信息熵为 0），此时的信息熵较小。

### 模型

数据结构，树

### ID3（迭代的二分器）

1. 目标：挑选信息增益最大的属性，优先选择。

#### 信息增益

1. 总的信息熵-其中某个特征值的信息熵就是信息增益。

### C4.5（ID3的后继）


### CART（与ID3大约同时独立地发明）


## 二分类（LR）

## 逻辑回归

### 模型

权重

## 深度学习

### 模型

隐向量
