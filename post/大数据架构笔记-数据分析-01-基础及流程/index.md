# 大数据架构笔记-数据分析


## 挑战
1. 在尝试使用数据挖掘的方法来解决业务问题，却发现巨大挑战不在于算法本身，而是`数据如何清洗、特征如何设计、效果如何优化`，这些与业务极度相关的问题。

## 结构(关键是思维)

### 基础知识

#### WHAT
1. 数据挖掘就是寻找数据中隐含的知识并用于产生商业价值。

#### WHY
1. 需要一种规范的解决方案，能够利用并且充分利用这些数据里的`每一个部分(维度众多)`，通过一些自动化的机器学习算法，从数据中自动提取价值。
2. 数据挖掘就提供了这样一系列的框架、工具和方法，可以处理不同类型的`大量数据`，并且使用`复杂`的算法部署，去探索数据中的模式。

#### TODO
1. 分类问题
2. 聚类问题:聚类的算法比较适合一些不确定的类别场景。
3. 回归问题:最大特点是，生成的结果是连续的，而不像分类和聚类生成的是一种离散的结果。
4. 关联问题:最常见的一个场景就是`推荐`.

#### HOW
1. 应用最多的 CRISP-DM（Cross-industry Standard Process for Data Mining，跨行业数据挖掘标准流程）方法论.
```
1. 业务理解（Business Understanding）从业务出发，到业务中去
2. 数据理解（Data Understanding）数据理解阶段始于数据的收集工作，重点是在业务理解的基础上，对我们所掌握的数据要有一个清晰、明确的认识，了解有哪些数据、哪些数据可能对目标有影响、哪些可能是冗余数据、哪些数据存在不足或缺失.
3. 数据准备（Data Preparation）基于原始数据，去构建数据挖掘模型所需的数据集的所有工作，包括数据收集、数据清洗、数据补全、数据整合、数据转换、特征提取等一系列动作。
4. 构建模型（Modeling）对于同一个数据挖掘的问题类型，可以有多种方法选择使用。如果有多重技术要使用，那么在这一任务中，对于每一个要使用的技术要分别对待。一些建模方法对数据的形式有具体的要求
5. 评估模型（Evaluation）评估之后会有两种情况，一种是评估通过，进入到上线部署阶段；另一种是评估不通过，那么就要反过来再进行迭代更新
6. 模型部署（Deployment）部署是一个挖掘项目的结束，也是一个数据挖掘项目的开始.
```
2. 在大多数的数据挖掘项目中，数据准备是最困难、最艰巨的一步。

### 数据挖掘过程

#### 理解业务和数据

##### 思想问题
1. 避免对业务的轻视。
```
1. 真正理解业务场景与挖掘需求.
2. 要做一个成功的数据挖掘项目，就要去`深入学习业务`，明白业务的关键点，在项目的`需求阶段与业务方进行充分的沟通`，在`发现偏差时及时调整`，甚至在制定 OKR 的时候也与业务方来共同制定，这样在做项目的时候才不会出现南辕北辙的问题。
```
2. 明白可以为和不可以为
```
1. 明白了业务的要求以及目标，还需要明白数据挖掘要解决的点在哪里。数据挖掘并不是万能的。
2. 第一个是数据不完美。从总体上来看，似乎我们的数据量很大、很充足，但数据的真实性、准确性、完整性具体到每一条数据的时候或者某一个需求的时候，是不完美的，甚至是匮乏的。你要解决这些问题需要付出大量的工作，甚至超出了业务本身，这就会造成入不敷出的情况，这个项目开展的必要性可能就要受到质疑了。
3. 第二个是业务条件不完美。
4. 数据挖掘只能在有限的资源与条件下去提供最大化的解决方案
5. 需要与业务方进行深入的沟通，同时对你所掌握的数据有充分的认识，对业务的难点和重点有明确的区分。建立需求多方评估机制，让业务专家与技术专家参与进来，评估需求的合理性以及你的数据情况，确认问题是否可以通过数据挖掘得到有效解决；或者是对需求进行拆解，以最大化在数据限制和业务限制前提下的项目效果。
```

##### 业务背景与目标
1. 数据挖掘是一种方法，需求的产生必然是因为`某种分析需求、某个问题或者某个业务目标的需求`。如果一开始就不能对问题进行准确的定位，那么后面该如何使用合适的数据选择合适的算法，都是无稽之谈。
2. 数据挖掘工程师自行去理解业务，那很可能出现偏差，和业务方的需求产生分歧。所以这时就应该展开沟通，并成立专家小组来对目标进行评审。

##### 把握数据
1. 这一步有点类似可行性分析。看得到的数据能否支撑挖掘需求。
2. 从粗粒度到细粒度，对数据的认知
```
1. 是否有数据
2. 有多少数据
3. 是什么样的数据:有哪些属性可以被用到.
4. 标签:针对特定的项目，比如说有监督学习任务
```

#### 准备数据（开始收集、处理数据）
1. 这个环节看起来好像是一个非核心环节，实际上在整个过程中却是最重要、最耗时的环节。

##### 找到数据
1. 这一步需要`掌握一些数据库的使用技巧`，如常用的关系型数据库 MySQL、大数据使用的 Hbase、Hive、搜索引擎数据库 ES、内存数据库 Redis，还有图数据库，如 Neo4j 或者 JanusGraph等，甚至还要跟各种业务部门沟通协商以获取数据.
2. 找到数据之后，需要进行简单的整理，如用统一的 id 把数据整合在一起等。

##### 数据探索
1. 为了尽可能获得足够多的特征，要对数据进行`分析、预处理以及转换`等基础工作，以构建出更加贴合你所要预测结果的特征，这使得数据维度大量扩展，所以我把这个环节叫作把`数据变多`或者`数据升维`。
2. 可以看到你的数据是否存在问题，比如`异常值、数据的偏差、缺失`，等等。如果是`数值型`的数据，还可以通过计算`均值、方差、中位数、标准差、最大值、最小值`等去探索、扩展。

##### 数据清洗(最烦琐、最头疼)
1. 处理扩展后的数据、解决所发现的问题，同时又要顾及处理后的数据是否适合应用于下一个步骤，所以我也把这一步骤称作把`数据变少`。

###### 缺失值的处理
1. 需要区分这些数据缺失的情况，因为`有些是业务所允许的缺失`，而有些则是`错误情况`导致的。
2. 关于缺失值的处理，一般就 3 种情况：删掉有缺失值的数据；补充缺失值；不做处理。当然这些处理方式也依赖于数据是否可以被补充、缺失值是否重要，以及你所选用的算法能否处理缺失的情况等因素。

###### 异常值的处理
1. 异常值通常说的是那些与样本空间中绝大多数数据分布`差距过大`的数据：错误的情况(需要对数据进行修正，或者直接丢弃)和正常的情况（需要重视，分析后边的问题.需要根据你的业务需求进行处理。如果你的目标就是发现异常情况，那么这种异常值就需要保留下来，甚至需要特别关照。如果你的目标跟这些异常值没有关系，那么可以对这些异常值做一些修正，比如限定最大值和最小值的标准等，从而防止这些数据影响你后面模型的效果）。

###### 数据偏差的处理
1. 没有什么数据是非常对等和均衡的，越是天然的数据越是符合正态分布的规律。
2. 数据偏差可能导致后面训练的模型过拟合或者欠拟合.
3. 如果你需要比较均衡的样本，那么通常可以考虑丢弃较多的数据，或者补充较少的数据。在补充较少的数据时，又可以考虑使用现有数据去合成一些数据，或者直接复制一些数据从而增加样本数量。当然了，每一种方案都有它的优点和缺点，具体的情况还是要根据目标来决定，哪个对目标结果的影响较小就采取哪种方案。

###### 数据标准化
1. 防止某个维度的数据因为数值的差异，而对结果产生较大的影响。
2. 在有些算法中，每一个维度的数据标准都需要进行统一；而在另外一些算法中，则需要统一数据的类型。
3. 归一化。

###### 特征选择
1. 尽可能留下较少的数据维度，而又可以不降低模型训练的效果。
2. 维度越多，数据就会越稀疏，模型的可解释性就会变差、可信度降低。过多维度还会造成运算的缓慢。同时那些多余的维度可能会对模型的结果产生不好的影响。
3. 需要用到特征选择的技巧，比如自然语言处理里的关键词提取，或者`去掉屏蔽词`，以减少不必要的数据维度。对于数值型的数据，可以使用`主成分分析`等算法来进行特征选择。

##### 构建训练集与测试集
1. 在训练之前，你要把数据分成训练集和测试集，有些还会有验证集。
2. 如果是均衡的数据，即各个分类的数据量基本一致，可以直接随机抽取一定比例的数据作为训练样本，另外一部分作为测试样本。
3. 如果是非均衡的数据，比如在风控型挖掘项目中，风险类数据一般远远少于普通型数据，这时候使用`分层抽样`以保障每种类型的数据都可以出现在训练集和测试集中。
4. 训练集和测试集的构建也是有方法的:
```
留出法，就是直接把整个数据集划分为两个互斥的部分，使得训练集和测试集互不干扰，这个是最简单的方法，适合大多数场景；
交叉验证法，先把数据集划分成 n 个小的数据集，每次使用 n-1 个数据集作为训练集，剩下的作为测试集进行 n 次训练，这种方法主要是为了训练多个模型以降低单个模型的随机性；
自助法，通过重复抽样构建数据集，通常在小数据集的情况下非常适用。
```

##### 思想准备
1. 准备数据可能是数据挖掘所有环节中，最苦、最累、耗时最长的一环了.
2. 而且当你的模型出现错误，结果达不到预期，往往需要重新回到数据准备环节进行处理，反复迭代几次最终才能达到你期望的目标。

#### 数据建模（如何选择算法）模型训练
1. 首先你得明白你面对的是什么问题，虽然算法众多，但是要`解决的难题往往有共同点`，针对每一类型的问题，就可以找到对应的算法，再根据算法的特性去进行选择。

##### 分类问题（寻找决策边界）
1. 分类是有监督的学习过程（首先要有一批已经有标签结果的数据）。
2. 【二分类】只有“是”或“否”。
3. 【多分类】 在二分类的基础上，将标签可选范围扩大。
4. 【多标签分类】对于二分类和多分类，一条内容最后的结果只有一个，标签之间是互斥的关系。多标签分类下的一条数据可以被标注上多个标签。
5. 【算法】像 KNN 算法、决策树算法、随机森林、SVM 等都是为解决分类问题设计的。

##### 聚类问题
1. 聚类是无监督的。
2. 聚类就是把一个数据集划分成多个组的过程，使得组内的数据尽量高度集中，而和其他组的数据之间尽量远离。这种方法是针对已有的数据进行划分，不涉及未知的数据。

###### 互斥
1. 一个元素只存在于一个小组。
2. 【第一种：基于划分的聚类，通常用于互斥的小组】

###### 相交
2. 一条数据可能既存在于 A 组，也存在于 B 组之中。
2. 【第二种：基于密度的聚类，可以用来解决数据形状不均匀的情况。】有些数据集分布并不均匀，而是呈现不规则的形状，而且组和组之间有一片空白区域，这个时候用划分的方法就很难处理，但是基于密度的聚类不会受到分布形状的影响，只是根据数据的紧密程度去聚类。

###### 层次
1. 一个大组还可以细分成若干个小组。
2. 【第三种：基于层级的聚类，适用于需要对数据细分的情况。】要把数据按照层次进行分组，可以使用自顶向下的方法，使得全部数据只有一个组，然后再分裂成更小的组，直到满足你的要求。如有从属关系，需要细分的数据，就非常适合这种方法。同样，也可以使用自底向上的方法，最开始每一条数据都是一个组，然后把离得近的组合并起来，直到满足条件。

###### 模糊
1. 一个用户并不绝对属于某个小组，只是用概率来表示他和某个小组的关系。假设有五个小组，那么他属于这五个小组的模糊关系就是[0.5,0.5,0.4,0.2,0.7]。
2. 【最后一种：基于模型的聚类。】首先假设我们的数据符合某种概率分布模型，比如说高斯分布或者正态分布， 那么对于每一种类别都会有一个分布曲线，然后按照这个概率分布对数据进行聚类，从而获得模糊聚类的关系。

##### 回归问题（找到最优拟合）
1. 与分类问题十分相似.监督学习。分类方法输出的是离散的标签，回归方法输出的结果是连续值。
2. 回归就是要通过拟合数据找到一个函数。这条线可能不通过任何一个数据点，而是使得所有数据点到这条线的距离都是最短的，或者说是损失最小的。
3. 回归方法和分类方法可以相互转化。在使用回归方法得到函数方程式以后，你可以根据对新数据运算的结果进行区间分段，高于某个阈值给定一个标签，低于该阈值给定另外一个标签；相反，对于通过分类方法得到的标签，你可以根据给定标签的概率值为其增加一些运算逻辑，将标签转换到一个连续值的结果上。。

##### 关联问题
1. 关联分析。无监督学习。
2. 【目标】是挖掘隐藏在数据中的关联模式并加以利用。关联分析是要在已有的数据中寻找出数据的相关关系，以期望能够使用这些规则去提升效率和业绩。
3. 关联分析被广泛地用于各种商品销售分析、相关推荐系统分析、用户行为分析等情况。
4. 但是在进行大量数据的关联分析时，你会发现各种奇怪的组合，这可能是数据偏差产生的影响，所以在最终结果应用的时候还需要加入一些`知识校验`。

##### 模型集成
1. 在实践的时候，很多问题不是靠一个算法、一个模型就能解决的，往往要针对具体的细节使用多个模型以获得最佳效果，所以就要用到模型集成。
2. 模型集成也可以叫作集成学习，其思路就是去合并多个模型来提升整体的效果。

###### Bagging（装袋法）
1.  比如多次随机抽样构建训练集，每构建一次，就训练一个模型，最后对多个模型的结果附加一层决策，使用平均结果作为最终结果。随机森林算法就运用了该方法.

###### Boosting（增强法）
1. 这个就是串行的训练，即每次把上一次训练的结果也作为一个特征，不断地强化学习的效果。

###### Stacking（堆叠法）
1. 这个方法比较宽泛，它对前面两种方法进行了扩展，训练的多个模型既可以进行横向扩展，也可以进行串行增强，最终再使用分类或者回归的方法把前面模型的结果进行整合。其中的每一个模型可以使用不同的算法，对于结构也没有特定的规则.

#### 模型评估（确认达标,主要适用于分类模型等监督学习的模型）
1. 模型评估就是对你的模型进行多种维度的评估，来确认你的模型是否可以放到线上去使用。

##### 评估指标

###### 混淆矩阵与准确率指标(模型评估时最受关注的指标)
1. 混淆矩阵中包含以下 4 种数值：
```
1. 真阳性（True Positive，TP）：样本的真实类别是正例，并且模型预测的结果也是正例。
2. 真阴性（True Negative，TN）：样本的真实类别是负例，并且模型将其预测成为负例。
3. 假阳性（False Positive，FP）：样本的真实类别是负例，但是模型将其预测成为正例。
4. 假阴性（False Negative，FN）：样本的真实类别是正例，但是模型将其预测成为负例。
```
2. 根据上述混淆矩阵，我们可以计算一些数值:
```
准确率（Accuracy）：是指所有预测正确的占全部样本的概率，公式为 (TP+TN)/(TP+FP+FN+TN)。
精确率（Precision）：指的是预测正确的结果占所有预测成“是”的概率，即 TP/(TP+FP)。精确率按照类别来计算。
召回率（Recall）：按照类别来区分，某个类别结果的召回率即该类别下预测正确的结果占该类别所有数据的概率，即 TP/(TP+FN)。
F 值（F Score）：基于精确率和召回率的一个综合指标，是精确率和召回率的调和平均值。一般的计算方法是 2*（Precision*Recall）/（Precision+Recall）。如果一个模型的准确率为 0，召回率为 1，那么 F 值仍然为 0。
ROC 曲线和 AUC 值：这个略微有点复杂，但也是一个非常常用的指标。仍然是基于混淆矩阵，但不同的是这个对指标进行了细化，构建了很多组混淆矩阵。
真正例率：TP/（TP+FN）
假正例率：FP/（FP+TN）
```
3. 在有些模型的产出中，通常给出“是”和“否”的概率值（这两个概率值相加为 1），我们根据概率值来判定最终的结果.
4. 在坐标系上画出一系列的点，纵坐标是`真正例率`，横坐标是`假正例率`，把这些点连起来形成的曲线就是 `ROC 曲线`（Receiver Operating Characteristic，接收者操作特征）。ROC 曲线下方的面积是 `AUC 值`（Area Under Curve，曲线下面积），ROC 曲线和 AUC 值可以反映一个模型的`稳定性`，当 ROC 曲线接近对角线时，说明模型输出很不稳定，模型就越不准确。

###### 十分重要的【业务抽样】评估
1. 在理想的状况下，使用上面的指标基本可以判定模型的效果，但是在实际中还存在着一些问题，这通常都是由数据本身并不完美导致的。对于标注数据，人工标注通常也存在一定的错误率，而不是 100% 正确，所以在使用前面的指标进行评估的时候可能会与实际结果存在一些差异。进行业务抽样评估可以减弱这种情况，在多方背靠背评估之后再进行意见的统一，最终得到一个在业务上认可的准确率情况。

###### 泛化能力评估
1. 泛化能力反映的是模型对未知数据的判断能力。
2. 在数据挖掘中，数据的维度通常有很多，而且数据也都是非标准值，任意记录之间的数据都会存在着差异，所以泛化能力好的模型在数据存在着波动的情况下，仍然能够做出正确的判断。
3. 我们通过两个指标可以评估模型的泛化能力是好还是坏，那就是过拟合（overfitting）和欠拟合（underfitting）。
4. 关于泛化性能的评估，主要依赖于在不同的数据集上的准确结果之间的比较。要处理过拟合和欠拟合的问题，通常需要对我们的数据进行重新整理，总结出现过拟合和欠拟合的原因，比如`是否数据量太少、数据维度不够丰富、数据本身的准确性较差`等，然后调整数据重新进行训练。

####### 过拟合(`判断条件过于苛刻`)：模型在训练集上表现良好，而在测试集或者验证集上表现不佳。

####### 欠拟合：在训练集和测试集上的表现都不好。

###### 其他评估指标
1. 模型速度：主要评估模型在处理数据上的开销和时间。
2. 鲁棒性：主要考虑在出现`错误数据`或者`异常数据`甚至是`数据缺失`时，模型是否可以给出正确的结果，甚至是否可以给出结果，会不会导致模型运算的崩溃。
3. 可解释性：随着机器 学习算法越来越复杂，尤其是在深度学习中，模型的可解释性越来越成为一个问题。由于在很多场景下（比如金融风控），需要给出一个让人信服的理由，所以可解释性也是算法研究的一大重点。

##### 评估数据的处理
1. 随机抽样：即最简单的一次性处理，把数据分成训练集与测试集，使用测试集对模型进行测试，得到各种准确率指标。
2. 随机多次抽样：在随机抽样的基础上，进行 n 次随机抽样，这样可以得到 n 组测试集，使用 n 组测试集分别对模型进行测试，那么可以得到 n 组准确率指标，使用这 n 组的平均值作为最终结果。
3. 交叉验证：交叉验证与随机抽样的区别是，交叉验证需要训练多个模型。譬如，k 折交叉验证，即把原始数据分为 k 份，每次选取其中的一份作为测试集，其他的作为训练集训练一个模型，这样会得到 k 个模型，计算这 k 个模型结果作为整体获得的准确率。
4. 自助法：自助法也借助了随机抽样和交叉验证的思想，先随机有放回地抽取样本，构建一个训练集，对比原始样本集和该训练集，把训练集中未出现的内容整理成为测试集。重复这个过程 k 次、构建出 k 组数据、训练 k 个模型，计算这 k 个模型结果作为整体获得的准确率，该方法比较适用于样本较少的情况。

#### 模型应用（是否解决了业务需求）

##### 模型部署

###### 模型的保存
1. 要制定好模型保存的规范，包括存放的位置、名字的定义、模型所使用的算法、参数、数据、效果等内容，防止发生比如遗忘、丢失、误删除，甚至是服务器崩坏等人为的事故，造成不必要的损失。

###### 模型的优化
1. 在模型部署应用阶段的很多限制条件在模型训练阶段并不会显现出来，模型训练阶段优化所追求的目标是效果要尽量好；而在模型应用阶段优化所追求的目标是在效果尽量不降低的前提下，适配应用的限制。
```
1. 在对时延要求比较高的场景下，如果业务应用无法忍受模型的响应时间，那么我们就需要想办法解决，是增加机器还是降低模型的复杂度以提高速度；
2. 在对模型大小要求比较高的场景下，我们期望把人脸识别模型部署到一个摄像装置的小型存储芯片上面，那么模型的大小就会受到限制，需要考虑降低模型的参数维度等。
```

###### 离线应用还是在线应用？
1. 想想我们的业务需求，是否要实时响应。

##### 项目总结

###### 记录项目经历，学会总结和反思
1. 总结的内容包括：从项目的需求发起，到数据准备，再到模型训练、评估、上线，这些环节都遇到了什么样的问题，我们解决了什么问题，又有哪些问题尚未解决，如果在时间等条件充裕的情况下还可以做哪些尝试。同时认真地做一下反思，把整个项目中的重点知识内化成自己的能力。

###### 多考虑一点，如何适合更多场景
1. 当完成了一个又一个需求之后，会发现很多需求似曾相识，但是又总有不一样的点。所以我们的数据挖掘模型或结果能不能做成统一的服务，能不能应用在更多的地方？
2. 多考虑一点就可以把数据挖掘前置到业务需求之前，最终形成完整的业务数据闭环，而不需要再进行冗余的开发。

##### 监控与迭代
1. 为了我们的模型保持良好的效果，需要有一份迭代计划去维护和更新我们的模型。

###### 模型的监控
1. 要了解什么时候该做出调整，我们需要有一套监控的策略。对于模型输出的结果进行监控，以查看模型在线上运行的效果是否符合预期，是否有比较大的变化。
2. 模型的监控可以从三个方面入手，分别是`结果监控`、`人工定期复审`以及`Case 收集与样本积累`。

####### 结果监控
1. 结果监控主要是针对一些具体的指标进行监控，包括我们在评估环节用到的准确率、召回率等，另外还可以根据具体产出的结果在业务中的效果进行监控。
```
1. 首先可以想到的是针对每天新闻的分类标签进行排名统计，来查看每个标签的占比情况与我们的初始数据是否接近；还可以监控到一些数据较少的类别是否能够被预测出来，这属于稳定性指标。
2. 其次，在推荐系统中，我们还会有 CTR（点击率预估）这样的总体指标，可以对标签与 CTR 的关系进行计算，来查看每个标签的 CTR 情况。
3. 在一些 App 中还会有主动负反馈，让用户自己选择不喜欢的标签。通过分析这些数据可以知道我们的模型结果是否还符合业务场景，是否需要做出一些调整。在一些长期的大型项目中，我们甚至需要建设一些 Debug 平台，对这些数据进行持续的、可视化的监控，观察每天每周的变化情况。
```

####### 人工定期复审
1. 该方法主要针对业务需求准确率的情况进行评估，查看当前的模型效果是否还满足业务的需求，准确率情况是否有所变化，同时也可以跟业务进行沟通评估，确认当前的情况是否需要对模型进行重新训练。

####### Case(样本) 收集与样本积累
1. 在整个监控的过程中，也需要进行 Case 的收集。

###### 时刻准备好重新开始

### 算法详解
