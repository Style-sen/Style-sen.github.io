# 大数据架构笔记-数据分析-02-算法简介


## 分类算法

### 最近邻算法（K-NearestNeighbor）KNN：近朱者赤，近墨者黑
1. 找到 K 个与新数据最近的样本，取样本中最多的一个类别作为新数据的类别。
2. 【距离最近】关于距离该怎么计算呢？最常见的一个计算方法就是欧式距离，即两点之间的连线，如果放在地图上就是两个房子的直线距离。当然除了欧式距离，还有很多距离计算的方式，比如曼哈顿距离、切比雪夫距离等。

#### 优缺点

##### 优点
1. 【简单易实现】KNN 算法最后实际上并没有抽象出任何模型，而是把全部的数据集直接当作模型本身，当一条新数据来了之后跟数据集里面的每一条数据进行对比。
2. 【对于边界不规则的数据效果较好】最终的预测是把未知数据作为中心点，然后画一个圈，使得圈里有 K 个数据，所以对于边界不规则的数据，要比线性的分类器效果更好。

##### 缺点
1. 【只适合小数据集】每次预测新数据都需要使用全部的数据集。
2. 【数据不平衡效果不好】特别多的数据最后在投票的时候会更有竞争优势。
3. 【必须要做数据标准化】使用距离来进行计算，如果数据量纲不同，数值较大的字段影响就会变大，所以需要对数据进行标准化，比如都转换到 0-1 的区间。
4. 【不适合特征维度太多的数据】  只能处理小数据集，如果数据的维度太多，那么样本在每个维度上的分布就很少。比如我们只有三个样本，每个样本只有一个维度，这比每个样本有三个维度特征要明显很多。

#### 关于 K（距离最近的样本数） 的选取
1. 当K 越小的时候容易过拟合；K 越大的时候容易欠拟合。
```
对于 K 的取值，一种显而易见的办法就是从 1 开始不断地尝试，查看准确率。随着 K 的增加，一般情况下准确率会先变大后变小，然后选取效果最好的那个 K 值就好了。
当然，关于 K 最好使用奇数，因为偶数在投票的时候就困难了，如果两个类别的投票数量是一样的，那就没办法抉择了，只能随机选一个。
```

### 决策树

#### 算法原理
1. 把所有样本数据中出现的情况组合都构建入这棵树的时候，我们的算法也就完成了对样本的学习。最终形成的这棵树上，所有的叶子节点都是要输出的类别信息，所有的非叶子节点都是特征信息。当一个新的数据来了之后，就按照对应的判断条件，从根节点走到叶子节点，从而获得这个数据的分类结果。
2. 【如何选择一个特征作为根节点？下一次决策又该选取哪个特征作为节点？】决策树算法使用了一种称作信息增益的方法来衡量一个特征和特征之间的重要性，信息增益越大表明这个特征越重要，那么就优先对这个特征进行决策。
3. 实际上决策树实现的时候都采用了贪心算法，来寻找一个最近的最优解，而不是全局的最优解。

#### 算法优缺点

##### 不同版本
1. 决策树最初的版本称为 ID3（ Iterative Dichotomiser 3 ），ID3 的缺点是无法处理数据是连续值的情况，也无法处理数据存在缺失的问题，需要在准备数据环节把缺失字段进行补齐或者删除数据。后来有人提出了改进方案称为 C4.5，加入了对连续值属性的处理，同时也可以处理数据缺失的情况。同时，还有一种目前应用最多的 CART（ Classification And Regression Tree）分类与回归树，每次分支只使用二叉树划分，同时可以用于解决回归问题。

##### CART优点
1. 【非常直观，可解释极强】:在生成的决策树上，每个节点都有明确的判断分支条件，所以非常容易看到为什么要这样处理，比起神经网络模型的黑盒处理，高解释性的模型非常受金融保险行业的欢迎。
2. 【预测速度比较快】 由于最终生成的模型是一个树形结构，对于一条新数据的预测，只需要按照条件在每一个节点进行判定就可以。通常来说，树形结构都有助于提升运算速度。
3. 【既可以处理离散值也可以处理连续值】还可以处理缺失值。

##### CART缺点
1. 【容易过拟合】在极端的情况下，我们根据样本生成了一个最完美的树，那么样本中出现的每一个值都会有一条路径来拟合，所以如果样本中存在一些问题数据，或者样本与测试数据存在一定的差距时，就会看出泛化性能不好，出现了过拟合的现象。
2. 【需要处理样本不均衡的问题】如果样本不均衡，某些特征的样本比例过大，最终的模型结果将会更偏向这些特征。
3. 【样本的变化会引发树结构巨变】

#### 关于剪枝
1. 【决策树容易过拟合】需要使用剪枝的方式来使得模型的泛化能力更好，所以剪枝可以理解为简化我们的决策树，去掉不必要的节点路径以提高泛化能力。剪枝的方法主要有预剪枝和后剪枝两种方式。
2. 【预剪枝】在决策树构建之初就设定一个阈值，当分裂节点的熵阈值小于设定值的时候就不再进行分裂了；然而这种方法的实际效果并不是很好，因为谁也没办法预料到我们设定的恰好是我们想要的。
3. 【后剪枝】后剪枝方法就是在我们的决策树已经构建完成以后，再根据设定的条件来判断是否要合并一些中间节点，使用叶子节点来代替。在实际的情况下，通常都是采用后剪枝的方案。

#### 扩展
1. 【随机森林】为了更好地解决泛化及树结构变动等问题，从决策树演进出来随机森林算法。根据我们前面讲的模型集成方法，随机森林就是使用了 bagging 方案构建了多棵决策树，然后对所有树的结果来进行平均计算以获得最终的结果。
2. 【GBDT】在随机森林的基础上，研究者又提出了梯度提升决策树算法（Gradient Boosting Decision Tree，GBDT），GBDT 是基于 boosting 的策略。与随机森林一样的是，GBDT 也会构建多棵决策树；但不同的是，GBDT 构建的多棵树之间是有联系的，每个分类器在上一轮分类器的残差基础上进行训练。
3. 【XGBoost】一个非常火热的模型，有“机器学习大杀器”之称，在很多比赛中都获得了非常好的结果。但实际上 XGBoost 不算是一个算法，而是对 GBDT 的一种工程实现，它优化了 GBDT 里面的求解过程，并加入了很多工程上的优化项目，使得数据处理、运算速度等环节都有了很大的提升。

### 朴素贝叶斯
