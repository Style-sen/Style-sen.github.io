# 大数据架构笔记-4-NLP-中文分词



## TF-IDF

1. 阮大神的[TF-IDF与余弦相似](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)系列文章。
2. 优点是简单快速，结果比较符合实际情况。缺点是，单纯以"词频"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法`无法体现词的位置信息`，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同。
3. 为后续的（bm25、word2vec，中文分词）做铺垫。

### 术语

1. `词频`（Term Frequency，缩写为TF）。如果某个词很重要，它应该在文章中多次出现。
2. `停用词`（stop words）。对找到结果毫无帮助、必须过滤掉的词。的 是 在
3. `常见词`
4. `少见词`
5. 如果某个词比较少见，但是它在文章中多次出现，那么它很可能就反映了这篇文章的特性，正是所需要的关键词。
6. 用统计学语言表达，就是在词频的基础上，要对每个词分配一个"重要性"权重。
7. 这个权重叫做`逆文档频率`（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。
8. 知道了"词频"（TF）和"逆文档频率"（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。

### 算法

#### 计算TF

#### 计算IDF

1. 需要一个语料库，计算IDF。

#### 计算TFIDF

1. TFxIDF。
2. TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。

### 使用

1. 自动提取关键词。
2. `搜索`每个文档，都可以分别计算一组搜索词（"中国"、"蜜蜂"、"养殖"）的TF-IDF，将它们相加，就可以得到`整个文档的TF-IDF`。这个值最高的文档就是与搜索词最相关的文档。
3. [自动摘要](http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html)：句子的信息量用"关键词"来衡量。如果包含的关键词越多，就说明这个句子越重要。

## 相似度

### 文本相似（可能语义不相似）

1. 基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。
2. LCS最大公共子序列。
3. 利用中文分词。

#### 余弦相似性（cosine similiarity）

1. 通过两个向量夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。
2. 余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。

#### 算法（word2Vec）

##### 第一步：分词（找出关键词）

##### 第二步：列出所有的词（注意顺序）（去重）

##### 第三步：计算词频

##### 第四步：写出词频向量（与第二步的顺序一致）

##### 第五步：计算词频向量的相似度

#### 应用

1. `找出相似文章`。
```
（1）使用TF-IDF算法，找出两篇文章的关键词；
（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；
（3）生成两篇文章各自的词频向量；
（4）计算两个向量的余弦相似度，值越大就表示越相似。
```

### 语义相似（可能文本不相似）需要依靠用户行为

1. 基于共现的行为（推荐-协同过滤）。
2. 基于共现的窗口（word2vec词嵌入模型）。

## 分词

1. 上述说明，关键是`分词`。`分词的好坏`，直接关系后续的算法。
2. 不存在完美的分词方法。`看业务`。
3. 搜索引擎：通过搜索找到更多内容，中文分词粒度细。
4. 推荐系统：为让用户更好地理解内容，中文分词粒度粗。

### 基础理论

#### 分词表达方案（计算机表达方式）

首先，一个句子中每个`字`都有一个index。
1. bit位表示。 大数据/架构/笔记  1001010
2. 序列表示。  大数据/架构/笔记  {0,3,5}
3. jieba表示。  大数据/架构/笔记  BMEBEBE  （S：single   B：begin    M：middle    E：End）

#### 常见分词方法

1. 参考[浅谈分词算法](https://www.cnblogs.com/xlturing/p/8465965.html)
2. 分词中涉及到三个基本问题：`分词规范`（即所谓Golden Data，大家所有的模型都在`统一的数据集`上进行训练和评测，这样比较才会具有可参考性。`不同的业务场景有差异`）、`歧义切分`和`未登录词的识别`。
3. `歧义切分`梁南元最早对歧义字段进行了两种基本的定义：【交集型切分歧义】：汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。此时汉字串J称作交集串。如，大学生（大学/学生）、研究生物（研究生/生物）、结合成（结合/合成）.【组合型切分歧义】：汉字串AB称作多义组合型切分歧义，如果满足A、B、AB同时为词。如，起身（他|站|起|身|来/明天|起身|去北京）、学生会（我在|学生会|帮忙/我的|
4. 想要正确的做出切分判断，一定要结合上下文语境，甚至韵律、语气、重音、停顿等。
5. `未登录词`，一种是指已有的词表中没有收录的词，另一种是指训练语料中未曾出现过的词。而后一种含义也可以被称作集外词，OOV（out of vocabulary），即训练集以外的词。通常情况下未登录词和OOV是一回事，我们这里不加以区分。
6. 【新出现的普通词汇】，一般对于大规模数据的分词系统，会专门集成一个新词发现模块，用于对新词进行挖掘发现，经过验证后加入到词典当中。【专有名词】，在分词系统中我们有一个专门的模块，命名体识别（NER name entity recognize），用于对人名、地名以及组织机构名等单独进行识别。【专业名词和研究领域名称】，这个在通用分词领域出现的情况比较少，如果出现特殊的新领域，专业，就会随之产生一批新的词汇。【其他专用名词】，包含其他新产生的产品名、电影、书籍等等。
7. 分词模型对于未登录词的处理将是衡量一个系统好坏的重要指标。

##### 基于词典分词（机械分词）通常作为初分手段

1. 基于词典的方法是经典的传统分词方法，这种方式很直观，我们`从大规模的训练语料中提取分词词库，并同时将词语的词频统计出来`，我们可以通过`逆向最大匹配、N-最短路径以及N-Gram模型`等分词方法对句子进行切分。
1. 词典是语料库（前向查找语料库和后向查找语料库）。Trie树
2. `最大长度查找`（前{正}向查找从前往后、后{反}向查找从后往前）
3. 有时前向查找和后向查找完全不一样。北京大学生活动中心

###### 概率语言模型（最大概率分词）方法

1. 切分词图（表示所有组词（语料库）可能）(有向无环图)
```
DAG表示{0:[0,1,3],1:[1],2:[2,3,5],3:[3],4:[4,5],5:[5],6:[6,7],7:[7]}   表示` 广州本田雅阁汽车`
```
1. 既然有多种组词可能，哪一种比较好，需要计算概率，选出概率最大的分词方式。
2. 从统计思想的角度看，分词问题的输入是一个字串C=c1,c2......cn 输出是一个词串S=w1,w2,w3.....wm m<=n.对于一个特定的字符串C，会与多个分词方案S对应，分词的任务就是在这些S中找出一个切分方案S，实测P(S|C)的`值`最大。
3. P(S|C) ∝ P(S)*P(C|S) 用自然语言来说就是 这种分词方式（词串）的可能性 乘以 这个词串生成我们的句子的可能性。P(C|S)恒等1.
4. `问题变成求P(S)的概率即P(W1, W2, W3, W4 ..)`。
5. 根据联合概率的公式展开：P(W1, W2, W3, W4 ..) = P(W1) * P(W2|W1) * P(W3|W2, W1) * P(W4|W1,W2,W3) * .. 于是我们可以通过一系列的条件概率（右式）的乘积来求整个联合概率。然而不幸的是随着条件数目的增加（P(Wn|Wn-1,Wn-2,..,W1) 的条件有 n-1 个），数据稀疏问题也会越来越严重，即便语料库再大也无法统计出一个靠谱的 P(Wn|Wn-1,Wn-2,..,W1) 来。为了缓解这个问题，计算机科学家们一如既往地使用了“天真”假设：我们假设句子中一个词的出现概率只依赖于它前面的有限的 k 个词（k 一般不超过 3，如果只依赖于前面的一个词，就是2元语言模型（2-gram），同理有 3-gram 、 4-gram 等），`这个就是所谓的“有限地平线”假设`。虽然这个假设很傻很天真，但结果却表明它的结果往往是很好很强大的，后面要提到的朴素贝叶斯方法使用的假设跟这个精神上是完全一致的，我们会解释为什么像这样一个天真的假设能够得到强大的结果。目前我们只要知道，有了这个假设，刚才那个乘积就可以改写成： P(W1) * P(W2|W1) * P(W3|W2) * P(W4|W3) .. （假设每个词只依赖于它前面的一个词）。而统计 P(W2|W1) 就不再受到数据稀疏问题的困扰了。对于我们上面提到的例子“南京市长江大桥”，如果按照自左到右的贪婪方法分词的话，结果就成了“南京市长/江大桥”。但如果按照贝叶斯分词的话（假设使用 3-gram），由于“南京市长”和“江大桥”在语料库中一起出现的频率为 0 ，这个整句的概率便会被判定为 0 。 从而使得“南京市/长江大桥”这一分词方式胜出。
6. 我们最多只看到前两个词，有研究表明，大于4个以上的模型并不会取得更好的效果（显然n越大，我们需要找寻n元组的词出现的频率就越低，会很直接的导致数据稀疏问题），通常情况下我们使用的是2-gram模型居多。


###### 贝叶斯

1. [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
2. P(B|A) = P(AB) / P(A) 其实这个就等于：P(B|A) * P(A) = P(AB)

###### 朴素贝叶斯

1. 我们假设 di 与 di-1 是完全条件无关的，于是式子就简化为 P(d1|h+) * P(d2|h+) * P(d3|h+) * .. 。这个就是所谓的条件独立假设，也正是朴素贝叶斯方法的朴素之处。
2. 有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大。

###### 层级贝叶斯模型

1. 隐马可夫模型（HMM）就是一个简单的层级贝叶斯模型。

##### 基于字分词

1. 不同于基于词典的分词方法，需要依赖于一个事先编制好的词典，通过查词典的方式作出最后的切分决策；`基于字的分词方法将分词过程看作是字的分类问题`，`其认为每个字在构造一个特定词语时都占据着一个确定的构词位置（词位）`。
2. 在实际生产环境中，我们往往会融合多种方法来提高准确率和召回率，比如在github中经常被提及的结巴分词就融合了n-gram词典分词和HMM字分词，大家具体用到的时候要根据实际环境进行选择和搭配，必要的时候要对模型进行重train和调整。
3. 马尔科夫模型是对一个序列数据建模，但有时需要对两个序列建模（机器翻译、语音识别、词性标注、拼音纠错等）。

###### hmm隐马模型

1. `状态值`集合Q={q1,q2,...,qN}，其中N为可能的状态数；
2. `观测值`集合V={v1,v2,...,vM}，其中M为可能的观测数；
3. `转移概率`矩阵A=[aij]，其中aij表示从状态i转移到状态j的概率；
4. `发射概率`矩阵（也称之为观测概率矩阵）B=[bj(k)]，其中bj(k)表示在状态j的条件下生成观测vk的概率；
5. `初始状态`分布π（状态的初始概率）.

####### 应用（3类问题）
评估问题：计算给定观测序列出现的概率。Forward-backward算法
解码问题：
6. 将状态值集合Q置为{B,E,M,S}，分别表示词的开始、结束、中间（begin、end、middle）及字符独立成词（single）；观测序列即为中文句子。比如，“今天天气不错”通过HMM求解得到状态序列“B E B E B E”，则分词结果为“今天/天气/不错”。
7. 先将句子分成单个字[浅谈分词算法（3）基于字的分词方法（HMM）](https://www.cnblogs.com/xlturing/p/8467033.html)
8. 给定观测序列，计算状态序列。
9. 用到了`viterbi`算法。
学习问题：模型参数未知，是观测序列的概率最大。极大似然估计的方法估计参数。BaumWelch  EM算法
###### CRF

###### LSTM

### jieba原理解析

#### 简介

1. 三种分词模式：精确模式（适合文本分析）、全模式（可以成词的词语都扫描出来）、搜索引擎模式（对长词再次切分）。
2. 支持繁体分词。
3. 支持自定义词典。
4. 基于`Trie`树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的`DAG`图。
5. 采用了动态规划查找最大概率路径，找出基于词频的最大切分组合。
6. 对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。

#### 分词步骤

1. 【准备工作】加载登录词典、建立Trie树分词模型（`之后弃用，Trie树嵌套dict，占内存。改成前缀词典`）。
2. 【一】得到句子，先进行清洗，如果包含特殊字符，将其分离。
3. 【二】建立分词的DAG词图（如{0:[0,1,3]}）。
4. 【三】计算全局概率Route（根据上方的DAG图），从右往左反向计算最大概率（`概率对数相加，防止相乘下溢`），得到基于前缀词典的词频最大切分组合。
5. 【四】Token识别，讲中文和英文数字分开处理。
6. 【五】对于未登录词，加载隐马HMM概率图模型。

### 分布式批量分词
